{"id":"a2deed74-7e61-42f2-8b82-5aaee600e652","data":{"nodes":[{"id":"ChatInput-5NXAd","type":"genericNode","position":{"x":-222,"y":-34.98749923706055},"data":{"id":"ChatInput-5NXAd","node":{"base_classes":["Message"],"beta":false,"conditional_paths":[],"custom_fields":{},"description":"Get chat inputs from the Playground.","display_name":"Chat Input","documentation":"","edited":false,"field_order":["input_value","sender","sender_name","session_id","files"],"frozen":false,"icon":"ChatInput","output_types":[],"outputs":[{"cache":true,"display_name":"Message","hidden":false,"method":"message_response","name":"message","selected":"Message","types":["Message"],"value":"__UNDEFINED__"}],"pinned":false,"template":{"_type":"Component","code":{"advanced":true,"dynamic":true,"fileTypes":[],"file_path":"","info":"","list":false,"load_from_db":false,"multiline":true,"name":"code","password":false,"placeholder":"","required":true,"show":true,"title_case":false,"type":"code","value":"from langflow.base.data.utils import IMG_FILE_TYPES, TEXT_FILE_TYPES\nfrom langflow.base.io.chat import ChatComponent\nfrom langflow.io import DropdownInput, FileInput, MessageTextInput, MultilineInput, Output\nfrom langflow.schema.message import Message\n\n\nclass ChatInput(ChatComponent):\n    display_name = \"Chat Input\"\n    description = \"Get chat inputs from the Playground.\"\n    icon = \"ChatInput\"\n\n    inputs = [\n        MultilineInput(\n            name=\"input_value\",\n            display_name=\"Text\",\n            value=\"\",\n            info=\"Message to be passed as input.\",\n        ),\n        DropdownInput(\n            name=\"sender\",\n            display_name=\"Sender Type\",\n            options=[\"Machine\", \"User\"],\n            value=\"User\",\n            info=\"Type of sender.\",\n            advanced=True,\n        ),\n        MessageTextInput(\n            name=\"sender_name\",\n            display_name=\"Sender Name\",\n            info=\"Name of the sender.\",\n            value=\"User\",\n            advanced=True,\n        ),\n        MessageTextInput(\n            name=\"session_id\", display_name=\"Session ID\", info=\"Session ID for the message.\", advanced=True\n        ),\n        FileInput(\n            name=\"files\",\n            display_name=\"Files\",\n            file_types=TEXT_FILE_TYPES + IMG_FILE_TYPES,\n            info=\"Files to be sent with the message.\",\n            advanced=True,\n            is_list=True,\n        ),\n    ]\n    outputs = [\n        Output(display_name=\"Message\", name=\"message\", method=\"message_response\"),\n    ]\n\n    def message_response(self) -> Message:\n        message = Message(\n            text=self.input_value,\n            sender=self.sender,\n            sender_name=self.sender_name,\n            session_id=self.session_id,\n            files=self.files,\n        )\n        if self.session_id and isinstance(message, Message) and isinstance(message.text, str):\n            self.store_message(message)\n            self.message.value = message\n\n        self.status = message\n        return message\n"},"files":{"advanced":true,"display_name":"Files","dynamic":false,"fileTypes":["txt","md","mdx","csv","json","yaml","yml","xml","html","htm","pdf","docx","py","sh","sql","js","ts","tsx","jpg","jpeg","png","bmp","image"],"file_path":"","info":"Files to be sent with the message.","list":true,"name":"files","placeholder":"","required":false,"show":true,"title_case":false,"trace_as_metadata":true,"type":"file","value":""},"input_value":{"advanced":false,"display_name":"Text","dynamic":false,"info":"Message to be passed as input.","input_types":["Message"],"list":false,"load_from_db":false,"multiline":true,"name":"input_value","placeholder":"","required":false,"show":true,"title_case":false,"trace_as_input":true,"trace_as_metadata":true,"type":"str","value":"helloo"},"sender":{"advanced":true,"display_name":"Sender Type","dynamic":false,"info":"Type of sender.","name":"sender","options":["Machine","User"],"placeholder":"","required":false,"show":true,"title_case":false,"trace_as_metadata":true,"type":"str","value":"User"},"sender_name":{"advanced":true,"display_name":"Sender Name","dynamic":false,"info":"Name of the sender.","input_types":["Message"],"list":false,"load_from_db":false,"name":"sender_name","placeholder":"","required":false,"show":true,"title_case":false,"trace_as_input":true,"trace_as_metadata":true,"type":"str","value":"User"},"session_id":{"advanced":true,"display_name":"Session ID","dynamic":false,"info":"Session ID for the message.","input_types":["Message"],"list":false,"load_from_db":false,"name":"session_id","placeholder":"","required":false,"show":true,"title_case":false,"trace_as_input":true,"trace_as_metadata":true,"type":"str","value":""}}},"type":"ChatInput"},"selected":false,"width":384,"height":308,"dragging":false},{"id":"Prompt-oAq30","type":"genericNode","position":{"x":293.5884267613458,"y":97.71452776934143},"data":{"description":"Create a prompt template with dynamic variables.","display_name":"Prompt","id":"Prompt-oAq30","node":{"base_classes":["Message"],"beta":false,"conditional_paths":[],"custom_fields":{"template":["user_input"]},"description":"Create a prompt template with dynamic variables.","display_name":"Prompt","documentation":"","edited":false,"error":null,"field_order":["template"],"frozen":false,"full_path":null,"icon":"prompts","is_composition":null,"is_input":null,"is_output":null,"name":"","output_types":[],"outputs":[{"cache":true,"display_name":"Prompt Message","hidden":false,"method":"build_prompt","name":"prompt","selected":"Message","types":["Message"],"value":"__UNDEFINED__"}],"pinned":false,"template":{"_type":"Component","code":{"advanced":true,"dynamic":true,"fileTypes":[],"file_path":"","info":"","list":false,"load_from_db":false,"multiline":true,"name":"code","password":false,"placeholder":"","required":true,"show":true,"title_case":false,"type":"code","value":"from langflow.base.prompts.api_utils import process_prompt_template\nfrom langflow.custom import Component\nfrom langflow.io import Output, PromptInput\nfrom langflow.schema.message import Message\nfrom langflow.template.utils import update_template_values\n\n\nclass PromptComponent(Component):\n    display_name: str = \"Prompt\"\n    description: str = \"Create a prompt template with dynamic variables.\"\n    icon = \"prompts\"\n    trace_type = \"prompt\"\n\n    inputs = [\n        PromptInput(name=\"template\", display_name=\"Template\"),\n    ]\n\n    outputs = [\n        Output(display_name=\"Prompt Message\", name=\"prompt\", method=\"build_prompt\"),\n    ]\n\n    async def build_prompt(\n        self,\n    ) -> Message:\n        prompt = await Message.from_template_and_variables(**self._attributes)\n        self.status = prompt.text\n        return prompt\n\n    def post_code_processing(self, new_build_config: dict, current_build_config: dict):\n        \"\"\"\n        This function is called after the code validation is done.\n        \"\"\"\n        frontend_node = super().post_code_processing(new_build_config, current_build_config)\n        template = frontend_node[\"template\"][\"template\"][\"value\"]\n        _ = process_prompt_template(\n            template=template,\n            name=\"template\",\n            custom_fields=frontend_node[\"custom_fields\"],\n            frontend_node_template=frontend_node[\"template\"],\n        )\n        # Now that template is updated, we need to grab any values that were set in the current_build_config\n        # and update the frontend_node with those values\n        update_template_values(frontend_template=frontend_node, raw_template=current_build_config[\"template\"])\n        return frontend_node\n"},"template":{"advanced":false,"display_name":"Template","dynamic":false,"info":"","list":false,"name":"template","placeholder":"","required":false,"show":true,"title_case":false,"trace_as_input":true,"type":"prompt","value":"Answer the user as if you were a pirate.\n\nUser: {user_input}\n\nAnswer: "},"user_input":{"advanced":false,"display_name":"user_input","dynamic":false,"field_type":"str","fileTypes":[],"file_path":"","info":"","input_types":["Message","Text"],"list":false,"load_from_db":false,"multiline":true,"name":"user_input","password":false,"placeholder":"","required":false,"show":true,"title_case":false,"type":"str","value":""}}},"type":"Prompt"},"selected":false,"width":384,"height":422,"dragging":false},{"id":"ChatOutput-GOfz9","type":"genericNode","position":{"x":1841.3113101551985,"y":171.8273242348505},"data":{"id":"ChatOutput-GOfz9","node":{"base_classes":["Message"],"beta":false,"conditional_paths":[],"custom_fields":{},"description":"Display a chat message in the Playground.","display_name":"Chat Output","documentation":"","edited":false,"field_order":["input_value","sender","sender_name","session_id","data_template"],"frozen":false,"icon":"ChatOutput","output_types":[],"outputs":[{"cache":true,"display_name":"Message","method":"message_response","name":"message","selected":"Message","types":["Message"],"value":"__UNDEFINED__"}],"pinned":false,"template":{"_type":"Component","code":{"advanced":true,"dynamic":true,"fileTypes":[],"file_path":"","info":"","list":false,"load_from_db":false,"multiline":true,"name":"code","password":false,"placeholder":"","required":true,"show":true,"title_case":false,"type":"code","value":"from langflow.base.io.chat import ChatComponent\nfrom langflow.io import DropdownInput, MessageTextInput, Output\nfrom langflow.schema.message import Message\n\n\nclass ChatOutput(ChatComponent):\n    display_name = \"Chat Output\"\n    description = \"Display a chat message in the Playground.\"\n    icon = \"ChatOutput\"\n\n    inputs = [\n        MessageTextInput(\n            name=\"input_value\",\n            display_name=\"Text\",\n            info=\"Message to be passed as output.\",\n        ),\n        DropdownInput(\n            name=\"sender\",\n            display_name=\"Sender Type\",\n            options=[\"Machine\", \"User\"],\n            value=\"Machine\",\n            advanced=True,\n            info=\"Type of sender.\",\n        ),\n        MessageTextInput(\n            name=\"sender_name\", display_name=\"Sender Name\", info=\"Name of the sender.\", value=\"AI\", advanced=True\n        ),\n        MessageTextInput(\n            name=\"session_id\", display_name=\"Session ID\", info=\"Session ID for the message.\", advanced=True\n        ),\n        MessageTextInput(\n            name=\"data_template\",\n            display_name=\"Data Template\",\n            value=\"{text}\",\n            advanced=True,\n            info=\"Template to convert Data to Text. If left empty, it will be dynamically set to the Data's text key.\",\n        ),\n    ]\n    outputs = [\n        Output(display_name=\"Message\", name=\"message\", method=\"message_response\"),\n    ]\n\n    def message_response(self) -> Message:\n        message = Message(\n            text=self.input_value,\n            sender=self.sender,\n            sender_name=self.sender_name,\n            session_id=self.session_id,\n        )\n        if self.session_id and isinstance(message, Message) and isinstance(message.text, str):\n            self.store_message(message)\n            self.message.value = message\n\n        self.status = message\n        return message\n"},"data_template":{"advanced":true,"display_name":"Data Template","dynamic":false,"info":"Template to convert Data to Text. If left empty, it will be dynamically set to the Data's text key.","input_types":["Message"],"list":false,"load_from_db":false,"name":"data_template","placeholder":"","required":false,"show":true,"title_case":false,"trace_as_input":true,"trace_as_metadata":true,"type":"str","value":"{text}"},"input_value":{"advanced":false,"display_name":"Text","dynamic":false,"info":"Message to be passed as output.","input_types":["Message"],"list":false,"load_from_db":false,"name":"input_value","placeholder":"","required":false,"show":true,"title_case":false,"trace_as_input":true,"trace_as_metadata":true,"type":"str","value":""},"sender":{"advanced":true,"display_name":"Sender Type","dynamic":false,"info":"Type of sender.","name":"sender","options":["Machine","User"],"placeholder":"","required":false,"show":true,"title_case":false,"trace_as_metadata":true,"type":"str","value":"Machine"},"sender_name":{"advanced":true,"display_name":"Sender Name","dynamic":false,"info":"Name of the sender.","input_types":["Message"],"list":false,"load_from_db":false,"name":"sender_name","placeholder":"","required":false,"show":true,"title_case":false,"trace_as_input":true,"trace_as_metadata":true,"type":"str","value":"AI"},"session_id":{"advanced":true,"display_name":"Session ID","dynamic":false,"info":"Session ID for the message.","input_types":["Message"],"list":false,"load_from_db":false,"name":"session_id","placeholder":"","required":false,"show":true,"title_case":false,"trace_as_input":true,"trace_as_metadata":true,"type":"str","value":""}}},"type":"ChatOutput"},"selected":false,"width":384,"height":308},{"id":"CombineText-00OQl","type":"genericNode","position":{"x":769.1556681177236,"y":183.72983308108564},"data":{"type":"CombineText","node":{"template":{"_type":"Component","code":{"type":"code","required":true,"placeholder":"","list":false,"show":true,"multiline":true,"value":"from langflow.custom import Component\nfrom langflow.io import MessageTextInput, Output\nfrom langflow.schema.message import Message\n\n\nclass CombineTextComponent(Component):\n    display_name = \"Combine Text\"\n    description = \"Concatenate two text sources into a single text chunk using a specified delimiter.\"\n    icon = \"merge\"\n    name = \"CombineText\"\n\n    inputs = [\n        MessageTextInput(\n            name=\"text1\",\n            display_name=\"First Text\",\n            info=\"The first text input to concatenate.\",\n        ),\n        MessageTextInput(\n            name=\"text2\",\n            display_name=\"Second Text\",\n            info=\"The second text input to concatenate.\",\n        ),\n        MessageTextInput(\n            name=\"delimiter\",\n            display_name=\"Delimiter\",\n            info=\"A string used to separate the two text inputs. Defaults to a whitespace.\",\n            value=\" \",\n        ),\n    ]\n\n    outputs = [\n        Output(display_name=\"Combined Text\", name=\"combined_text\", method=\"combine_texts\"),\n    ]\n\n    def combine_texts(self) -> Message:\n        combined = self.delimiter.join([self.text1, self.text2])\n        self.status = combined\n        return Message(text=combined)\n","fileTypes":[],"file_path":"","password":false,"name":"code","advanced":true,"dynamic":true,"info":"","load_from_db":false,"title_case":false},"delimiter":{"trace_as_input":true,"trace_as_metadata":true,"load_from_db":false,"list":false,"required":false,"placeholder":"","show":true,"value":" ","name":"delimiter","display_name":"Delimiter","advanced":false,"input_types":["Message"],"dynamic":false,"info":"A string used to separate the two text inputs. Defaults to a whitespace.","title_case":false,"type":"str"},"text1":{"trace_as_input":true,"trace_as_metadata":true,"load_from_db":false,"list":false,"required":false,"placeholder":"","show":true,"value":"","name":"text1","display_name":"First Text","advanced":false,"input_types":["Message"],"dynamic":false,"info":"The first text input to concatenate.","title_case":false,"type":"str"},"text2":{"trace_as_input":true,"trace_as_metadata":true,"load_from_db":false,"list":false,"required":false,"placeholder":"","show":true,"value":"","name":"text2","display_name":"Second Text","advanced":false,"input_types":["Message"],"dynamic":false,"info":"The second text input to concatenate.","title_case":false,"type":"str"}},"description":"Concatenate two text sources into a single text chunk using a specified delimiter.","icon":"merge","base_classes":["Message"],"display_name":"Combine Text","documentation":"","custom_fields":{},"output_types":[],"pinned":false,"conditional_paths":[],"frozen":false,"outputs":[{"types":["Message"],"selected":"Message","name":"combined_text","display_name":"Combined Text","method":"combine_texts","value":"__UNDEFINED__","cache":true,"hidden":false}],"field_order":["text1","text2","delimiter"],"beta":false,"edited":false},"id":"CombineText-00OQl"},"selected":false,"width":384,"height":523},{"id":"CustomComponent-yssIH","type":"genericNode","position":{"x":1308.149783342171,"y":415.61683040851494},"data":{"type":"CustomComponent","node":{"template":{"_type":"Component","api_key":{"load_from_db":false,"required":false,"placeholder":"","show":true,"value":"vyPLG1o2vfvnIjVrQYKfRhlCKqh58qSt","name":"api_key","display_name":"API Key","advanced":false,"input_types":[],"dynamic":false,"info":"The OpenAI API Key to use for the OpenAI model.","title_case":false,"password":true,"type":"str"},"code":{"type":"code","required":true,"placeholder":"","list":false,"show":true,"multiline":true,"value":"import json\nimport requests\nimport operator\nimport os\nfrom functools import reduce\n\nfrom langchain_openai import ChatOpenAI\nfrom pydantic.v1 import SecretStr\n\nfrom langflow.base.constants import STREAM_INFO_TEXT\nfrom langflow.base.models.model import LCModelComponent\nfrom langflow.field_typing import LanguageModel\nfrom langflow.inputs import (\n    BoolInput,\n    DictInput,\n    DropdownInput,\n    FloatInput,\n    IntInput,\n    MessageInput,\n    SecretStrInput,\n    StrInput,\n)\n\n\nclass ChatKserve_vLLM(LCModelComponent):\n    display_name = \"ChatKserve(vLLM runtime)\"\n    description = \"`Chat Kserve LLM instance served with vLLM runtime\"\n    icon = \"custom_components\"\n    name = \"CustomComponent\"\n\n    \n    \n    model_list = []\n    model_dict = {}\n\n    # APIを呼び出しKserveのモデル情報を取得\n    try:\n        # 任意のURLを指定\n        # http://model-gateway-api.hitachi-pj-{project_name}-core.svc.cluster.local/v1/models/{chat or embedding}\n        project_name = os.environ['APP_POD_NAMESPACE']\n        url = f\"http://model-gateway-api.{project_name}.svc.cluster.local/v1/models/chat\"\n\n        ret = requests.get(url)\n        if ret.status_code == 200:\n            json_data = ret.json()\n            models_list = json_data[\"models\"]\n            # Kserveのモデル名とエンドポイントURLを辞書データ化\n            for val in models_list:\n                # ステータスがservingのKserveが存在し、対象の値が空文字ではない場合\n\n                    model_dict = {val[\"name\"]: val[\"api_base\"]}\n                    model_list.append(val[\"name\"])\n            model_list.sort()\n        # モデル情報が存在しない場合\n        if model_list == []:\n            model_list = [\"dummy\"]\n    # 環境変数未設定エラー時の対応\n    except KeyError as ke:\n        print(ke)\n        model_list = [\"dummy\"]\n    # リクエストエラー時の対応\n    except Exception as e:\n        print(e)\n        model_list = [\"dummy\"]\n\n    inputs = [\n        MessageInput(\n            name=\"input_value\",\n            display_name=\"Input\"\n        ),\n        IntInput(\n            name=\"max_tokens\",\n            display_name=\"Max Tokens\",\n            advanced=True,\n            info=\"The maximum number of tokens to generate. Set to 0 for unlimited tokens.\",\n        ),\n        DictInput(\n            name=\"model_kwargs\",\n            display_name=\"Model Kwargs\",\n            advanced=True\n        ),\n        BoolInput(\n            name=\"json_mode\",\n            display_name=\"JSON Mode\",\n            advanced=True,\n            info=\"If True, it will output JSON regardless of passing a schema.\",\n        ),\n        DictInput(\n            name=\"output_schema\",\n            is_list=True,\n            display_name=\"Schema\",\n            advanced=True,\n            info=\"The schema for the Output of the model. You must pass the word JSON in the prompt. If left blank, JSON mode will be disabled.\",\n        ),\n        DropdownInput(\n            name=\"model_name\",\n            display_name=\"Model Name\",\n            advanced=False,\n            options=model_list,\n            value=model_list[0],\n            info=\"If \\\"dummy\\\" is displayed, the Model Name failed to be retrieved.\"\n        ),\n        StrInput(\n            name=\"endpoint\",\n            display_name=\"Endpoint\",\n            advanced=False,\n            show=False,\n            value=model_dict,\n            info=\"You can specify the model endpoint url rather than selet the model name.\",\n        ),\n        SecretStrInput(\n            name=\"api_key\",\n            display_name=\"API Key\",\n            info=\"The OpenAI API Key to use for the OpenAI model.\",\n            advanced=False,\n            value=\"API_KEY\",\n        ),\n        FloatInput(\n            name=\"temperature\",\n            display_name=\"Temperature\",\n            value=0.1\n        ),\n        BoolInput(\n            name=\"stream\",\n            display_name=\"Stream\",\n            info=STREAM_INFO_TEXT,\n            advanced=True\n        ),\n        StrInput(\n            name=\"system_message\",\n            display_name=\"System Message\",\n            info=\"System message to pass to the model.\",\n            advanced=True,\n        ),\n        IntInput(\n            name=\"seed\",\n            display_name=\"Seed\",\n            info=\"The seed controls the reproducibility of the job.\",\n            advanced=True,\n            value=1,\n        ),\n    ]\n\n    def build_model(self) -> LanguageModel:  # type: ignore[type-var]\n        # self.output_schea is a list of dictionarie s\n        # let's convert it to a dictionary\n        output_schema_dict: dict[str, str] = reduce(operator.ior, self.output_schema or {}, {})\n        api_key = self.api_key\n        temperature = self.temperature\n        model_name: str = self.model_name\n        max_tokens = self.max_tokens or 256\n        model_kwargs = self.model_kwargs or {}\n        json_mode = bool(output_schema_dict) or self.json_mode\n        seed = self.seed\n        model_kwargs[\"seed\"] = seed\n\n        project_name = os.environ['APP_POD_NAMESPACE']\n        if model_name:\n            endpoint = f\"http://model-gateway-kserve.{project_name}.svc.cluster.local/kserve/models/{model_name}/v1\"\n\n        dum = \"/mnt/models/\"\n        if api_key:\n            api_key = SecretStr(api_key)\n        else:\n            api_key = None\n        output = ChatOpenAI(\n            max_tokens=max_tokens or None,\n            model_kwargs=model_kwargs,\n            model=dum,\n            base_url=endpoint,\n            api_key=api_key,\n            temperature=temperature or 0.1,\n        )\n        if json_mode:\n            if output_schema_dict:\n                output = output.with_structured_output(schema=output_schema_dict, method=\"json_mode\")  # type: ignore\n            else:\n                output = output.bind(response_format={\"type\": \"json_object\"})  # type: ignore\n\n        return output  # type: ignore\n\n    def _get_exception_message(self, e: Exception):\n        \"\"\"\n        Get a message from an OpenAI exception.\n\n        Args:\n            exception (Exception): The exception to get the message from.\n\n        Returns:\n            str: The message from the exception.\n        \"\"\"\n\n        try:\n            from openai import BadRequestError\n        except ImportError:\n            return\n        if isinstance(e, BadRequestError):\n            message = e.body.get(\"message\")  # type: ignore\n            if message:\n                return message\n        return\n","fileTypes":[],"file_path":"","password":false,"name":"code","advanced":true,"dynamic":true,"info":"","load_from_db":false,"title_case":false},"endpoint":{"trace_as_metadata":true,"load_from_db":false,"list":false,"required":false,"placeholder":"","show":false,"value":{},"name":"endpoint","display_name":"Endpoint","advanced":false,"dynamic":false,"info":"You can specify the model endpoint url rather than selet the model name.","title_case":false,"type":"str"},"input_value":{"trace_as_input":true,"trace_as_metadata":true,"load_from_db":false,"list":false,"required":false,"placeholder":"","show":true,"value":"","name":"input_value","display_name":"Input","advanced":false,"input_types":["Message"],"dynamic":false,"info":"","title_case":false,"type":"str"},"json_mode":{"trace_as_metadata":true,"list":false,"required":false,"placeholder":"","show":true,"value":false,"name":"json_mode","display_name":"JSON Mode","advanced":true,"dynamic":false,"info":"If True, it will output JSON regardless of passing a schema.","title_case":false,"type":"bool"},"max_tokens":{"trace_as_metadata":true,"list":false,"required":false,"placeholder":"","show":true,"value":"","name":"max_tokens","display_name":"Max Tokens","advanced":true,"dynamic":false,"info":"The maximum number of tokens to generate. Set to 0 for unlimited tokens.","title_case":false,"type":"int"},"model_kwargs":{"trace_as_input":true,"list":false,"required":false,"placeholder":"","show":true,"value":{},"name":"model_kwargs","display_name":"Model Kwargs","advanced":true,"dynamic":false,"info":"","title_case":false,"type":"dict"},"model_name":{"trace_as_metadata":true,"options":["Claude 3 Haiku","gpt-35-turbo-16k","gpt-4o","gpt-4o","gpt-4o-mini","llama-3-swallow-8b-instruct-v01","modelkservellm"],"required":false,"placeholder":"","show":true,"value":"llama-3-swallow-8b-instruct-v01","name":"model_name","display_name":"Model Name","advanced":false,"dynamic":false,"info":"If \"dummy\" is displayed, the Model Name failed to be retrieved.","title_case":false,"type":"str","load_from_db":false},"output_schema":{"trace_as_input":true,"list":true,"required":false,"placeholder":"","show":true,"value":{},"name":"output_schema","display_name":"Schema","advanced":true,"dynamic":false,"info":"The schema for the Output of the model. You must pass the word JSON in the prompt. If left blank, JSON mode will be disabled.","title_case":false,"type":"dict"},"seed":{"trace_as_metadata":true,"list":false,"required":false,"placeholder":"","show":true,"value":1,"name":"seed","display_name":"Seed","advanced":true,"dynamic":false,"info":"The seed controls the reproducibility of the job.","title_case":false,"type":"int"},"stream":{"trace_as_metadata":true,"list":false,"required":false,"placeholder":"","show":true,"value":false,"name":"stream","display_name":"Stream","advanced":true,"dynamic":false,"info":"Stream the response from the model. Streaming works only in Chat.","title_case":false,"type":"bool"},"system_message":{"trace_as_metadata":true,"load_from_db":false,"list":false,"required":false,"placeholder":"","show":true,"value":"","name":"system_message","display_name":"System Message","advanced":true,"dynamic":false,"info":"System message to pass to the model.","title_case":false,"type":"str"},"temperature":{"trace_as_metadata":true,"list":false,"required":false,"placeholder":"","show":true,"value":0.1,"name":"temperature","display_name":"Temperature","advanced":false,"dynamic":false,"info":"","title_case":false,"type":"float"}},"description":"`Chat Kserve LLM instance served with vLLM runtime","icon":"custom_components","base_classes":["LanguageModel","Message"],"display_name":"ChatKserve(vLLM runtime)","documentation":"","custom_fields":{},"output_types":[],"pinned":false,"conditional_paths":[],"frozen":false,"outputs":[{"types":["Message"],"selected":"Message","name":"text_output","display_name":"Text","method":"text_response","value":"__UNDEFINED__","cache":true,"hidden":false},{"types":["LanguageModel"],"selected":"LanguageModel","name":"model_output","display_name":"Language Model","method":"build_model","value":"__UNDEFINED__","cache":true}],"field_order":["input_value","max_tokens","model_kwargs","json_mode","output_schema","model_name","endpoint","api_key","temperature","stream","system_message","seed"],"beta":false,"edited":true},"id":"CustomComponent-yssIH","description":"`Chat Kserve LLM instance served with vLLM runtime","display_name":"ChatKserve(vLLM runtime)"},"selected":false,"width":384,"height":647,"dragging":false,"positionAbsolute":{"x":1308.149783342171,"y":415.61683040851494}}],"edges":[{"source":"ChatInput-5NXAd","target":"Prompt-oAq30","sourceHandle":"{œdataTypeœ:œChatInputœ,œidœ:œChatInput-5NXAdœ,œnameœ:œmessageœ,œoutput_typesœ:[œMessageœ]}","targetHandle":"{œfieldNameœ:œuser_inputœ,œidœ:œPrompt-oAq30œ,œinputTypesœ:[œMessageœ,œTextœ],œtypeœ:œstrœ}","id":"reactflow__edge-ChatInput-5NXAd{œdataTypeœ:œChatInputœ,œidœ:œChatInput-5NXAdœ,œnameœ:œmessageœ,œoutput_typesœ:[œMessageœ]}-Prompt-oAq30{œfieldNameœ:œuser_inputœ,œidœ:œPrompt-oAq30œ,œinputTypesœ:[œMessageœ,œTextœ],œtypeœ:œstrœ}","data":{"sourceHandle":{"dataType":"ChatInput","id":"ChatInput-5NXAd","name":"message","output_types":["Message"]},"targetHandle":{"fieldName":"user_input","id":"Prompt-oAq30","inputTypes":["Message","Text"],"type":"str"}},"selected":false,"className":""},{"source":"Prompt-oAq30","target":"CombineText-00OQl","sourceHandle":"{œdataTypeœ:œPromptœ,œidœ:œPrompt-oAq30œ,œnameœ:œpromptœ,œoutput_typesœ:[œMessageœ]}","targetHandle":"{œfieldNameœ:œtext1œ,œidœ:œCombineText-00OQlœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}","id":"reactflow__edge-Prompt-oAq30{œdataTypeœ:œPromptœ,œidœ:œPrompt-oAq30œ,œnameœ:œpromptœ,œoutput_typesœ:[œMessageœ]}-CombineText-00OQl{œfieldNameœ:œtext1œ,œidœ:œCombineText-00OQlœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}","data":{"targetHandle":{"fieldName":"text1","id":"CombineText-00OQl","inputTypes":["Message"],"type":"str"},"sourceHandle":{"dataType":"Prompt","id":"Prompt-oAq30","name":"prompt","output_types":["Message"]}},"selected":false,"className":""},{"source":"CombineText-00OQl","sourceHandle":"{œdataTypeœ:œCombineTextœ,œidœ:œCombineText-00OQlœ,œnameœ:œcombined_textœ,œoutput_typesœ:[œMessageœ]}","target":"CustomComponent-yssIH","targetHandle":"{œfieldNameœ:œinput_valueœ,œidœ:œCustomComponent-yssIHœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}","data":{"targetHandle":{"fieldName":"input_value","id":"CustomComponent-yssIH","inputTypes":["Message"],"type":"str"},"sourceHandle":{"dataType":"CombineText","id":"CombineText-00OQl","name":"combined_text","output_types":["Message"]}},"id":"reactflow__edge-CombineText-00OQl{œdataTypeœ:œCombineTextœ,œidœ:œCombineText-00OQlœ,œnameœ:œcombined_textœ,œoutput_typesœ:[œMessageœ]}-CustomComponent-yssIH{œfieldNameœ:œinput_valueœ,œidœ:œCustomComponent-yssIHœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}","className":""},{"source":"CustomComponent-yssIH","sourceHandle":"{œdataTypeœ:œCustomComponentœ,œidœ:œCustomComponent-yssIHœ,œnameœ:œtext_outputœ,œoutput_typesœ:[œMessageœ]}","target":"ChatOutput-GOfz9","targetHandle":"{œfieldNameœ:œinput_valueœ,œidœ:œChatOutput-GOfz9œ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}","data":{"targetHandle":{"fieldName":"input_value","id":"ChatOutput-GOfz9","inputTypes":["Message"],"type":"str"},"sourceHandle":{"dataType":"CustomComponent","id":"CustomComponent-yssIH","name":"text_output","output_types":["Message"]}},"id":"reactflow__edge-CustomComponent-yssIH{œdataTypeœ:œCustomComponentœ,œidœ:œCustomComponent-yssIHœ,œnameœ:œtext_outputœ,œoutput_typesœ:[œMessageœ]}-ChatOutput-GOfz9{œfieldNameœ:œinput_valueœ,œidœ:œChatOutput-GOfz9œ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}","className":""}],"viewport":{"x":0,"y":0,"zoom":1}},"description":"Design, Develop, Dialogize.","name":"kserve_llm_flow","last_tested_version":"0.0.83","endpoint_name":null,"is_component":false}